{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from numpy import genfromtxt\n",
    "#my_data = genfromtxt('train.csv', delimiter=',', usecols=np.arange(0,12))\n",
    "train_data = pd.read_csv('train.csv', delimiter=',')\n",
    "test_data = pd.read_csv('test.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nChecklist for data analysis\\n[] Data cleaning\\n    [X] Data splitting and cleaning out unusable value\\n    [X] Specific data cleaning for certain model (maybe)\\n[] Model analysis\\n    [X] Logistic Regression\\n        80 % accuracy without cross validation for hyper parameter\\n        82 % from scikit with L2 regression with default parameter\\n    [] Nearest Neighbor\\n    [] Support Vector Machine\\n    [] Decision Tree analysis \\n        [] Ensemble method\\n        [] Random forest (bagging)\\n[] cross validation and model tunning --> WIP\\n    [] Data splitting method and analysis\\n    [] Hyper-Parameter analysis on certain models\\n[] Insight analysis\\n    [] Reverse engineering for missing data  --> WIP\\n        [] Re-run previous model to see improvement\\n[] Future work\\nIncludes graphic to show the analysis of the data\\n'"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Checklist for data analysis\n",
    "[] Data cleaning\n",
    "    [X] Data splitting and cleaning out unusable value\n",
    "    [X] Specific data cleaning for certain model (maybe)\n",
    "[] Model analysis\n",
    "    [X] Logistic Regression\n",
    "        80 % accuracy without cross validation for hyper parameter\n",
    "        82 % from scikit with L2 regression with default parameter\n",
    "    [] Nearest Neighbor\n",
    "    [] Support Vector Machine\n",
    "    [] Decision Tree analysis \n",
    "        [] Ensemble method\n",
    "        [] Random forest (bagging)\n",
    "[] cross validation and model tunning --> WIP\n",
    "    [] Data splitting method and analysis\n",
    "    [] Hyper-Parameter analysis on certain models\n",
    "[] Insight analysis\n",
    "    [] Reverse engineering for missing data  --> WIP\n",
    "        [] Re-run previous model to see improvement\n",
    "[] Future work\n",
    "Includes graphic to show the analysis of the data\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_data(data, percent):\n",
    "    train_size = len(data) * percent * 0.01\n",
    "    train_size = int(train_size)\n",
    "    return data.iloc[:train_size, :], data.iloc[train_size:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Turning catagory value using 1 hot and convert binary to 0/1 value\n",
    "Inital experiement, dropping name and ticket where it makes linear regression easier\n",
    "The cluster analysis should include such features to increase information gain\n",
    "''' \n",
    "def clean_linear_reg (train_data):\n",
    "    temp = train_data.copy()\n",
    "    #Features to modify or drop\n",
    "    temp.loc[:,'Cabin'] = temp['Cabin'].apply(lambda x : str(x)[:1])\n",
    "    temp.loc[:, 'Sex'] = temp['Sex'].apply(lambda x: 1 if x == \"male\" else 0)\n",
    "    temp = temp.drop(['Name','Ticket', 'PassengerId'], 1)\n",
    "    #temp = temp.drop('Ticket',1,)\n",
    "    #temp = temp.drop('PassengerId', 1)\n",
    "    #Apply one hot encoding for categorical data to analysis\n",
    "    temp = pd.get_dummies(temp)\n",
    "    #exclusive for titanic data\n",
    "    temp['Age'].fillna(temp['Age'].mean(), inplace=True)\n",
    "    cols = temp.columns.tolist()\n",
    "    cols.pop(0)\n",
    "    cols.append('Survived')\n",
    "    temp = temp[cols]\n",
    "    return temp\n",
    "\n",
    "def normalize(df):\n",
    "    result = df.copy()\n",
    "    for feature_name in df.columns:\n",
    "        max_value = df[feature_name].max()\n",
    "        min_value = df[feature_name].min()\n",
    "        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_data = clean_linear_reg(train_data)\n",
    "norm_data= normalize(clean_data)\n",
    "clean_train_norm, clean_test_norm = split_data(norm_data,80)\n",
    "#clean_train_norm = normalize(clean_data[0])\n",
    "#clean_test_norm = normalize(clean_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712, 19)\n",
      "(179, 19)\n"
     ]
    }
   ],
   "source": [
    "print clean_train_norm.shape\n",
    "print clean_test_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sigmod helper function for activation\n",
    "def sigmod(x):\n",
    "    if x < 0:\n",
    "        return 1 - 1/(1+math.exp(x))\n",
    "    else:\n",
    "        return (1.0 / ( 1 + math.exp(-1 * x)))\n",
    "\n",
    "f = []\n",
    "#Batch approach using linear regression method\n",
    "def batch_logistic_descent(data, step_size, tolerence, lambd, max_iteration=100, theta=None):\n",
    "    m = data.shape[0]\n",
    "    label = data.iloc[:,-1]\n",
    "    if theta is None:\n",
    "        theta = np.zeros(data.shape[1]-1, dtype=np.float)\n",
    "        print theta\n",
    "    for iteration in range (max_iteration+1):\n",
    "        h= (data.iloc[:,:-1].dot(theta)).apply(lambda x: sigmod(x))\n",
    "        \n",
    "        grads = (1.0/m) * ((data.iloc[:,:-1].T.dot(h - label)) + ( float(lambd) /m * theta))\n",
    "        print \"shape of grads\", grads.shape\n",
    "        theta = theta - (step_size * grads)\n",
    "        #print theta\n",
    "        errors = np.linalg.norm(h)\n",
    "        print \"Error norm\", errors\n",
    "        loss = np.linalg.norm(grads)\n",
    "        print \"grads loss\", loss\n",
    "        f.append(loss)\n",
    "        if iteration > max_iteration:\n",
    "            break\n",
    "    return theta            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subset = clean_data[0]\n",
    "#c = stochastic_gdescent(subset ,0.01,0.001, 1000)\n",
    "d = batch_logistic_descent(subset, 1, 0.01, 1, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.groupby('Survived').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#stochastic approach using linear regression method\n",
    "#Plot the value and compute for the average decision boundary of the survivor statstics\n",
    "def stochastic_gdescent (data, step_size, tolerence, max_iters=10000, theta=None):\n",
    "    label = data.iloc[:,-1]\n",
    "    if theta is None:\n",
    "        theta = np.zeros(data.shape[1]-1)\n",
    "    for x in range(0, max_iters):\n",
    "        print x\n",
    "        i = np.random.randint(low=0, high=data.shape[0], size=None)\n",
    "        error = theta.dot(data.iloc[i,:-1]) - label[i]\n",
    "        print \"Error \", error\n",
    "        for y in range(0,len(theta)):\n",
    "            delta_weight =  step_size * error * data.iloc[i,y]\n",
    "            theta[y] = theta[y] - delta_weight\n",
    "        #Report every 100 iteration to see the gradient and add data for plot\n",
    "        if (x % 100 == 0) :\n",
    "            #print \"Data point of x\" , x, \" is \", data.iloc[i]\n",
    "            print \"Iteration\", x\n",
    "            print \"Error in prediction at this point\", error\n",
    "            print \"Current theta\", theta\n",
    "            #a[x/100] = (np.sum(data.dot(theta) - label **2)/10000) #error (normalize)\n",
    "        if (error **2 < tolerence) and x > 200:\n",
    "            print \"Error reached termination condition\"\n",
    "            return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subset = clean_data[0].iloc[:10]\n",
    "#c = stochastic_gdescent(subset ,0.01,0.001, 1000)\n",
    "d = batch_logistic_descent(subset, 0.01, 0.001, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(clean_train_norm.iloc[:,:-1], clean_train_norm.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82122905027932958"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(clean_test_norm.iloc[:,:-1], clean_test_norm.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sigmod helper function for activation\n",
    "def sigmod(x):\n",
    "    if x < 0:\n",
    "        return 1 - 1/ (1+math.exp(x))\n",
    "    else:\n",
    "        return (1.0 / ( 1 + math.exp(-1 * x)))\n",
    "\n",
    "'''\n",
    "The logistic regression classifier fits default using L2 regularization\n",
    "Lamba and alpha are the hyper parameter to control the regularization and convergence rate\n",
    "The train data and label data can be any data type (numpy, panda.dataframe)\n",
    "'''\n",
    "def logistic_regression_fit (train_data, label, max_iterations, lamda, verbose=False, \n",
    "                         theta=None, threshold=0.001, alpha=0.01):\n",
    "    convergence_rate = []\n",
    "    if theta is None:\n",
    "        theta = np.zeros(train_data.shape[1], dtype=np.float)\n",
    "    n = train_data.shape[1] #size of the data\n",
    "    m = train_data.shape[0] #number of the features\n",
    "    for iteration in range(0, max_iterations):\n",
    "        hx = train_data.dot(theta).apply(lambda x: sigmod(x))\n",
    "        #print \"hx is \", hx\n",
    "        gradient = (1.0/m) * train_data.T.dot(hx - label) + (float(lamda)/m * theta)\n",
    "        #print \"gradient\", gradient\n",
    "        theta = theta - (alpha * gradient)\n",
    "        \n",
    "        loss = np.linalg.norm(gradient)\n",
    "        convergence_rate.append(loss)\n",
    "        if loss < threshold:\n",
    "            break;\n",
    "    '''\n",
    "    Plot and visualization of the converence rate. Verbose option will trigger this output\n",
    "    '''\n",
    "    if verbose is True:\n",
    "        print \"Total Iteration: \", iteration\n",
    "        print \"Final loss: \", loss\n",
    "        plt.plot(convergence_rate)\n",
    "    return theta, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Iteration:  9999\n",
      "Final loss:  0.0123302305159\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEACAYAAABcXmojAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/weOphizgGTnUQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6c5d4ee690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameter, loss = logistic_regression_fit(clean_train_norm.iloc[:,:-1], clean_train_norm.iloc[:,-1], 10000, 1,\n",
    "                                      verbose=True, threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def score (X, Y, weight):\n",
    "    '''\n",
    "    Helper function to score the accuracy of the model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape = (n_samples, n_features)\n",
    "        Test samples.\n",
    "\n",
    "    y : array-like, shape = (n_samples,)\n",
    "        True labels for X.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out : float\n",
    "        Accuracy of the model in %.\n",
    "    '''\n",
    "    \n",
    "    prediction = X.dot(weight).apply(lambda x: 1 if sigmod(x) > 0.5 else 0)\n",
    "    error_count = 0.0\n",
    "    for x in range(0, Y.shape[0]):\n",
    "        if prediction.iloc[x] != Y.iloc[x]:\n",
    "            error_count += 1\n",
    "    error = (1 - (error_count / float(m)))\n",
    "    print error\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(179, 19)\n",
      "(18,)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-370-d841339fd2ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mclean_test_norm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mparameter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_test_norm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclean_test_norm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-366-3dedfe448482>\u001b[0m in \u001b[0;36mscore\u001b[1;34m(X, Y, weight)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0merror_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0merror_count\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name 'm' is not defined"
     ]
    }
   ],
   "source": [
    "print clean_test_norm.shape\n",
    "print parameter.shape\n",
    "f = score(clean_test_norm.iloc[:,:-1], clean_test_norm.iloc[:,-1], parameter)\n",
    "print f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
