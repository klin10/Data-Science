{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from numpy import genfromtxt\n",
    "#my_data = genfromtxt('train.csv', delimiter=',', usecols=np.arange(0,12))\n",
    "train_data = pd.read_csv('train.csv', delimiter=',')\n",
    "test_data = pd.read_csv('test.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Checklist for data analysis\n",
    "[] Data cleaning\n",
    "    [X] Data splitting and cleaning out unusable value\n",
    "    [X] Specific data cleaning for certain model (maybe)\n",
    "[] Model analysis\n",
    "    [X] Logistic Regression\n",
    "    [] Nearest Neighbor\n",
    "    [] Support Vector Machine\n",
    "    [] Decision Tree analysis \n",
    "        [] Ensemble method\n",
    "        [] Random forest (bagging)\n",
    "[] cross validation and model tunning --> WIP\n",
    "    [] Data splitting method and analysis\n",
    "    [] Hyper-Parameter analysis on certain models\n",
    "[] Insight analysis\n",
    "    [] Reverse engineering for missing data  --> WIP\n",
    "        [] Re-run previous model to see improvement\n",
    "[] Future work\n",
    "Includes graphic to show the analysis of the data\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_data(data, percent):\n",
    "    train_size = len(data) * percent * 0.01\n",
    "    train_size = int(train_size)\n",
    "    return data.iloc[:train_size, :], data.iloc[train_size:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Turning catagory value using 1 hot and convert binary to 0/1 value\n",
    "Inital experiement, dropping name and ticket where it makes linear regression easier\n",
    "The cluster analysis should include such features to increase information gain\n",
    "''' \n",
    "def clean_linear_reg (train_data):\n",
    "    temp = train_data.copy()\n",
    "    #Features to modify or drop\n",
    "    temp.loc[:,'Cabin'] = temp['Cabin'].apply(lambda x : str(x)[:1])\n",
    "    temp.loc[:, 'Sex'] = temp['Sex'].apply(lambda x: 1 if x == \"male\" else 0)\n",
    "    temp = temp.drop(['Name','Ticket', 'PassengerId'], 1)\n",
    "    #temp = temp.drop('Ticket',1,)\n",
    "    #temp = temp.drop('PassengerId', 1)\n",
    "    #Apply one hot encoding for categorical data to analysis\n",
    "    temp = pd.get_dummies(temp)\n",
    "    #exclusive for titanic data\n",
    "    temp['Age'].fillna(temp['Age'].mean(), inplace=True)\n",
    "    cols = temp.columns.tolist()\n",
    "    cols.pop(0)\n",
    "    cols.append('Survived')\n",
    "    temp = temp[cols]\n",
    "    return temp\n",
    "\n",
    "def normalize(df):\n",
    "    result = df.copy()\n",
    "    for feature_name in df.columns:\n",
    "        max_value = df[feature_name].max()\n",
    "        min_value = df[feature_name].min()\n",
    "        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_data = clean_linear_reg(train_data)\n",
    "norm_data= normalize(clean_data)\n",
    "clean_train_norm, clean_test_norm = split_data(norm_data,80)\n",
    "#clean_train_norm = normalize(clean_data[0])\n",
    "#clean_test_norm = normalize(clean_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sigmod helper function for activation\n",
    "def sigmod(x):\n",
    "    if x < 0:\n",
    "        return 1 - 1/(1+math.exp(x))\n",
    "    else:\n",
    "        return (1.0 / ( 1 + math.exp(-1 * x)))\n",
    "\n",
    "f = []\n",
    "#Batch approach using linear regression method\n",
    "def batch_logistic_descent(data, step_size, tolerence, lambd, max_iteration=100, theta=None):\n",
    "    m = data.shape[0]\n",
    "    label = data.iloc[:,-1]\n",
    "    if theta is None:\n",
    "        theta = np.zeros(data.shape[1]-1, dtype=np.float)\n",
    "        print theta\n",
    "    for iteration in range (max_iteration+1):\n",
    "        h= (data.iloc[:,:-1].dot(theta)).apply(lambda x: sigmod(x))\n",
    "        \n",
    "        grads = (1.0/m) * ((data.iloc[:,:-1].T.dot(h - label)) + ( float(lambd) /m * theta))\n",
    "        print \"shape of grads\", grads.shape\n",
    "        theta = theta - (step_size * grads)\n",
    "        #print theta\n",
    "        errors = np.linalg.norm(h)\n",
    "        print \"Error norm\", errors\n",
    "        loss = np.linalg.norm(grads)\n",
    "        print \"grads loss\", loss\n",
    "        f.append(loss)\n",
    "        if iteration > max_iteration:\n",
    "            break\n",
    "    return theta            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subset = clean_data[0]\n",
    "#c = stochastic_gdescent(subset ,0.01,0.001, 1000)\n",
    "d = batch_logistic_descent(subset, 1, 0.01, 1, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.groupby('Survived').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#stochastic approach using linear regression method\n",
    "#Plot the value and compute for the average decision boundary of the survivor statstics\n",
    "def stochastic_gdescent (data, step_size, tolerence, max_iters=10000, theta=None):\n",
    "    label = data.iloc[:,-1]\n",
    "    if theta is None:\n",
    "        theta = np.zeros(data.shape[1]-1)\n",
    "    for x in range(0, max_iters):\n",
    "        print x\n",
    "        i = np.random.randint(low=0, high=data.shape[0], size=None)\n",
    "        error = theta.dot(data.iloc[i,:-1]) - label[i]\n",
    "        print \"Error \", error\n",
    "        for y in range(0,len(theta)):\n",
    "            delta_weight =  step_size * error * data.iloc[i,y]\n",
    "            theta[y] = theta[y] - delta_weight\n",
    "        #Report every 100 iteration to see the gradient and add data for plot\n",
    "        if (x % 100 == 0) :\n",
    "            #print \"Data point of x\" , x, \" is \", data.iloc[i]\n",
    "            print \"Iteration\", x\n",
    "            print \"Error in prediction at this point\", error\n",
    "            print \"Current theta\", theta\n",
    "            #a[x/100] = (np.sum(data.dot(theta) - label **2)/10000) #error (normalize)\n",
    "        if (error **2 < tolerence) and x > 200:\n",
    "            print \"Error reached termination condition\"\n",
    "            return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Batch approach using linear regression method\n",
    "def batch_gdescent(data, step_size, tolerence, max_iteration=10000, theta=None):\n",
    "    label = data.iloc[:,-1]\n",
    "    print label.shape\n",
    "    if theta is None:\n",
    "        theta = np.zeros(data.shape[1]-1)\n",
    "    not_convergence = True\n",
    "    gradient_magnitude = 0\n",
    "    iteration = 0\n",
    "    while not_convergence:\n",
    "        error = data.iloc[:,:-1].dot(theta) - label\n",
    "        print \"sum of error\", error.sum()\n",
    "        for y in range(0, len(theta)):\n",
    "            delta_weight = (error.dot(data.iloc[:,y]))\n",
    "            print \"delta sum\", delta_weight\n",
    "            theta[y] -= (step_size * delta_weight)\n",
    "            gradient_magnitude += delta_weight\n",
    "        print \"theta is\", theta\n",
    "        if gradient_magnitude ** 2 < tolerence:\n",
    "            print theta\n",
    "            not_convergence = False\n",
    "        iteration +=1\n",
    "        print \"iteration\", iteration\n",
    "        gradient_mangitude = 0\n",
    "        print \"error\" , error\n",
    "        if iteration > max_iteration:\n",
    "            break\n",
    "    return theta            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subset = clean_data[0].iloc[:10]\n",
    "#c = stochastic_gdescent(subset ,0.01,0.001, 1000)\n",
    "d = batch_logistic_descent(subset, 0.01, 0.001, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(subset.iloc[:-5,:-1], subset.iloc[:-5,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.densify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sigmod helper function for activation\n",
    "def sigmod(x):\n",
    "    if x < 0:\n",
    "        return 1 - 1/(1+math.exp(x))\n",
    "    else:\n",
    "        return (1.0 / ( 1 + math.exp(-1 * x)))\n",
    "\n",
    "'''\n",
    "The logistic regression classifier fits default using L2 regularization\n",
    "Lamba and alpha are the hyper parameter to control the regularization and convergence rate\n",
    "The train data and label data can be any data type (numpy, panda.dataframe)\n",
    "'''\n",
    "def logistic_regression_fit (train_data, label, max_iterations, lamda, verbose=False, \n",
    "                         theta=None, threshold=0.001, alpha=0.01):\n",
    "    convergence_rate = []\n",
    "    if theta is None:\n",
    "        theta = np.zeros(train_data.shape[1], dtype=np.float)\n",
    "    n = train_data.shape[1] #size of the data\n",
    "    m = train_data.shape[0] #number of the features\n",
    "    for iteration in range(0, max_iterations):\n",
    "        hx = train_data.dot(theta).apply(lambda x: sigmod(x))\n",
    "        #print \"hx is \", hx\n",
    "        gradient = (1.0/m) * train_data.T.dot(hx - label) + (float(lamda)/m * theta)\n",
    "        #print \"gradient\", gradient\n",
    "        theta = theta - (alpha * gradient)\n",
    "        \n",
    "        loss = np.linalg.norm(gradient)\n",
    "        convergence_rate.append(loss)\n",
    "        if loss < threshold:\n",
    "            break;\n",
    "    '''\n",
    "    Plot and visualization of the converence rate. Verbose option will trigger this output\n",
    "    '''\n",
    "    if verbose is True:\n",
    "        print \"Total Iteration: \", iteration\n",
    "        print \"Final loss: \", loss\n",
    "        plt.plot(convergence_rate)\n",
    "    print loss\n",
    "    return theta, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parameter, loss = logistic_regression(clean_train_norm.iloc[:,:-1], clean_train_norm.iloc[:,-1], 20, 1,\n",
    "                                      verbose=True, threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.275299828829\n",
      "Pclass       -0.025691\n",
      "Sex          -0.037826\n",
      "Age          -0.008643\n",
      "SibSp        -0.001768\n",
      "Parch        -0.000498\n",
      "Fare          0.000853\n",
      "Cabin_A       0.000162\n",
      "Cabin_B       0.002274\n",
      "Cabin_C       0.000907\n",
      "Cabin_D       0.001853\n",
      "Cabin_E       0.001440\n",
      "Cabin_F       0.000724\n",
      "Cabin_G       0.000007\n",
      "Cabin_T      -0.000138\n",
      "Cabin_n      -0.027201\n",
      "Embarked_C    0.001913\n",
      "Embarked_Q   -0.000965\n",
      "Embarked_S   -0.021062\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print loss\n",
    "print parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def score (theta, test_data):\n",
    "    m = test_data.shape[0]\n",
    "    n = test_data.shape[1]\n",
    "    print theta.shape\n",
    "    print \"m and n \", m, n\n",
    "    result = test_data.iloc[:,:-1].T.dot(theta).apply(lambda x: 1 if sigmod(x) else 0)\n",
    "    label = test_data.iloc[:, -1]\n",
    "    error_count = 0.0\n",
    "    for x in rarnge(0, m):\n",
    "        if result.iloc[x] != label[x]:\n",
    "            error_count +=0\n",
    "    return (error_count / m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_col_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print clean_test_norm.shape\n",
    "f = score(parameter,clean_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
