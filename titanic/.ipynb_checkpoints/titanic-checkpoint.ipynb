{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from logistic import logistic_regression\n",
    "from model_eval import model_eval\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv', delimiter=',')\n",
    "test_data = pd.read_csv('test.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "'''\n",
    "Checklist for data analysis\n",
    "[] Data cleaning\n",
    "    [X] Data splitting and cleaning out unusable value\n",
    "    [X] Specific data cleaning for certain model (maybe)\n",
    "[] Model analysis\n",
    "    [X] Logistic Regression\n",
    "        80 % accuracy without cross validation for hyper parameter\n",
    "        82 % from scikit with L2 regression with default parameter\n",
    "        Classifer Class function written\n",
    "    [X] Nearest Neighbor\n",
    "        Cross validation for a set of neighbor max k = 10\n",
    "    [] Support Vector Machine\n",
    "        Writting own implementation of the quadratic solve if possible\n",
    "    [X] Decision Tree analysis \n",
    "        [] Ensemble method\n",
    "        [] Random forest (bagging)\n",
    "[] cross validation and model tunning\n",
    "    [X] Data splitting method and analysis\n",
    "    [X] Hyper-Parameter analysis on certain models --> Cross validation with a set of parameter\n",
    "[] Insight analysis\n",
    "    [] Reverse engineering for missing data  --> WIP\n",
    "        [] Re-run previous model to see improvement\n",
    "[] Future work\n",
    "Includes graphic to show the analysis of the data\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Turning catagory value using 1 hot and convert binary to 0/1 value\n",
    "Inital experiement, dropping name and ticket where it makes linear regression easier\n",
    "The cluster analysis should include such features to increase information gain\n",
    "''' \n",
    "def clean_linear_reg (train_data):\n",
    "    temp = train_data.copy()\n",
    "    #Features to modify or drop\n",
    "    temp.loc[:,'Cabin'] = temp['Cabin'].apply(lambda x : str(x)[:1])\n",
    "    temp.loc[:, 'Sex'] = temp['Sex'].apply(lambda x: 1 if x == \"male\" else 0)\n",
    "    temp = temp.drop(['Name','Ticket', 'PassengerId'], 1)\n",
    "    #temp = temp.drop('Ticket',1,)\n",
    "    #temp = temp.drop('PassengerId', 1)\n",
    "    #Apply one hot encoding for categorical data to analysis\n",
    "    temp = pd.get_dummies(temp)\n",
    "    #exclusive for titanic data\n",
    "    temp['Age'].fillna(temp['Age'].mean(), inplace=True)\n",
    "    cols = temp.columns.tolist()\n",
    "    cols.pop(0)\n",
    "    cols.append('Survived')\n",
    "    temp = temp[cols]\n",
    "    return temp\n",
    "\n",
    "def normalize(df):\n",
    "    result = df.copy()\n",
    "    for feature_name in df.columns:\n",
    "        max_value = df[feature_name].max()\n",
    "        min_value = df[feature_name].min()\n",
    "        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result\n",
    "\n",
    "def split_data(data, percent):\n",
    "    train_size = len(data) * percent * 0.01\n",
    "    train_size = int(train_size)\n",
    "    return data.iloc[:train_size, :], data.iloc[train_size:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_data = clean_linear_reg(train_data)\n",
    "norm_data= normalize(clean_data)\n",
    "clean_train_norm, clean_test_norm = split_data(norm_data,80)\n",
    "#clean_train_norm = normalize(clean_data[0])\n",
    "#clean_test_norm = normalize(clean_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712, 19)\n",
      "(179, 19)\n"
     ]
    }
   ],
   "source": [
    "print clean_train_norm.shape\n",
    "print clean_test_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn import neural_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(clean_train_norm.iloc[:,:-1], clean_train_norm.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82122905027932958"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(clean_test_norm.iloc[:,:-1], clean_test_norm.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = logistic_regression()\n",
    "Y = clean_train_norm.iloc[:,-1]\n",
    "X = clean_train_norm.iloc[:,:-1]\n",
    "model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = model_eval(logistic_regression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8059071729957806"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.k_cross_validation(3,X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The logistic regression performs well with basic classification after normalization and conversion to numeric value.\n",
    "The NA value is filled by the mean of the column. The performance of the model does not seem to be affect by the number of missing data. Logsitic model is good at handling noises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81564245810055869"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Decision tree classifier\n",
    "decision_tree_classifer = tree.DecisionTreeClassifier()\n",
    "decision_tree_classifer.fit(clean_train_norm.iloc[:,:-1], clean_train_norm.iloc[:,-1])\n",
    "decision_tree_classifer.score(clean_test_norm.iloc[:,:-1], clean_test_norm.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83798882681564246"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#K-Nearest neighbor\n",
    "knn_classifer = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_classifer.fit(clean_train_norm.iloc[:,:-1], clean_train_norm.iloc[:,-1])\n",
    "knn_classifer.score(clean_test_norm.iloc[:,:-1], clean_test_norm.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84916201117318435"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Support Vector machine\n",
    "svm_classifer = svm.SVC(C=100)\n",
    "svm_classifer.fit(clean_train_norm.iloc[:,:-1], clean_train_norm.iloc[:,-1])\n",
    "svm_classifer.score(clean_test_norm.iloc[:,:-1], clean_test_norm.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-fbb0fb336b6f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mclassifer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclassifers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mclassifer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_train_norm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclean_train_norm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_test_norm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclean_test_norm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'classifers' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "result = []\n",
    "for classifer in classifers:\n",
    "    classifer.fit(clean_train_norm.iloc[:,:-1], clean_train_norm.iloc[:,-1])\n",
    "    score = classifer.score(clean_test_norm.iloc[:,:-1], clean_test_norm.iloc[:,-1])\n",
    "    result.append(classifer, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nltk version is 3.1.\n",
      "The scikit-learn version is 0.16.1.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "\n",
    "print('The nltk version is {}.'.format(nltk.__version__))\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "\n",
    "# The nltk version is 3.0.0.\n",
    "# The scikit-learn version is 0.15.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
