{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from numpy import genfromtxt\n",
    "#my_data = genfromtxt('train.csv', delimiter=',', usecols=np.arange(0,12))\n",
    "train_data = pd.read_csv('train.csv', delimiter=',')\n",
    "test_data = pd.read_csv('test.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nChecklist for data analysis\\n[] Data cleaning\\n    [X] Data splitting and cleaning out unusable value\\n    [X] Specific data cleaning for certain model (maybe)\\n    \\n[] Model analysis\\n    [X] Logistic Regression\\n    [] Nearest Neighbor\\n    [] Decision Tree analysis \\n        [] Ensemble method\\n        [] Random forest (bagging)\\n[] cross validation and model tunning\\n    [] Data splitting method and analysis\\n    [] Hyper-Parameter analysis on certain models\\n[] Insight analysis\\n    [] Reverse engineering for missing data\\n        [] Re-run previous model to see improvement\\n[] Future work\\nIncludes graphic to show the analysis of the data\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Checklist for data analysis\n",
    "[] Data cleaning\n",
    "    [X] Data splitting and cleaning out unusable value\n",
    "    [X] Specific data cleaning for certain model (maybe)\n",
    "    \n",
    "[] Model analysis\n",
    "    [X] Logistic Regression\n",
    "    [] Nearest Neighbor\n",
    "    [] Decision Tree analysis \n",
    "        [] Ensemble method\n",
    "        [] Random forest (bagging)\n",
    "[] cross validation and model tunning\n",
    "    [] Data splitting method and analysis\n",
    "    [] Hyper-Parameter analysis on certain models\n",
    "[] Insight analysis\n",
    "    [] Reverse engineering for missing data\n",
    "        [] Re-run previous model to see improvement\n",
    "[] Future work\n",
    "Includes graphic to show the analysis of the data\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_data(data, percent):\n",
    "    train_size = len(data) * percent * 0.01\n",
    "    train_size = int(train_size)\n",
    "    return data.iloc[:train_size, :], data.iloc[train_size:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Turning catagory value using 1 hot and convert binary to 0/1 value\n",
    "Inital experiement, dropping name and ticket where it makes linear regression easier\n",
    "The cluster analysis should include such features to increase information gain\n",
    "''' \n",
    "def clean_linear_reg (train_data, test_data):\n",
    "    c = [] \n",
    "    for temp in [train_data, test_data] : \n",
    "        #Features to modify or drop\n",
    "        temp.loc[:,'Cabin'] = temp['Cabin'].apply(lambda x : str(x)[:1])\n",
    "        temp.loc[:, 'Sex'] = temp['Sex'].apply(lambda x: 1 if x == \"male\" else 0)\n",
    "        print temp.shape\n",
    "        temp = temp.drop('Name', 1)\n",
    "        temp = temp.drop('Ticket',1,)\n",
    "        temp = temp.drop('PassengerId', 1)\n",
    "        print temp.shape\n",
    "        #Apply one hot encoding for categorical data to analysis\n",
    "        temp = pd.get_dummies(temp)\n",
    "        #exclusive for titanic data\n",
    "        temp['Age'].fillna(temp['Age'].mean(), inplace=True)\n",
    "        cols = temp.columns.tolist()\n",
    "        cols.pop(0)\n",
    "        cols.append('Survived')\n",
    "        temp = temp[cols]        \n",
    "        c.append(temp)\n",
    "    return c\n",
    "\n",
    "def normalize(df):\n",
    "    result = df.copy()\n",
    "    for feature_name in df.columns:\n",
    "        max_value = df[feature_name].max()\n",
    "        min_value = df[feature_name].min()\n",
    "        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a, b = split_data(train_data, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712, 12)\n",
      "(712, 9)\n",
      "(179, 12)\n",
      "(179, 9)\n"
     ]
    }
   ],
   "source": [
    "clean_data = clean_linear_reg(a, b)\n",
    "train_norm = normalize(clean_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sigmod helper function for activation\n",
    "def sigmod(x):\n",
    "    if x < 0:\n",
    "        return 1 - 1/(1+math.exp(x))\n",
    "    else:\n",
    "        return (1.0 / ( 1 + math.exp(-1 * x)))\n",
    "\n",
    "f = []\n",
    "#Batch approach using linear regression method\n",
    "def batch_logistic_descent(data, step_size, tolerence, lambd, max_iteration=100, theta=None):\n",
    "    m = data.shape[0]\n",
    "    label = data.iloc[:,-1]\n",
    "    if theta is None:\n",
    "        theta = np.zeros(data.shape[1]-1, dtype=np.float)\n",
    "        print theta\n",
    "    for iteration in range (max_iteration+1):\n",
    "        h= (data.iloc[:,:-1].dot(theta)).apply(lambda x: sigmod(x))\n",
    "        \n",
    "        grads = (1.0/m) * ((data.iloc[:,:-1].T.dot(h - label)) + ( float(lambd) /m * theta))\n",
    "        print \"shape of grads\", grads.shape\n",
    "        theta = theta - (step_size * grads)\n",
    "        #print theta\n",
    "        errors = np.linalg.norm(h)\n",
    "        print \"Error norm\", errors\n",
    "        loss = np.linalg.norm(grads)\n",
    "        print \"grads loss\", loss\n",
    "        f.append(loss)\n",
    "        if iteration > max_iteration:\n",
    "            break\n",
    "    return theta            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "shape of grads (18,)\n",
      "Error norm 13.3416640641\n",
      "grads loss 4.28780388354\n",
      "shape of grads (18,)\n",
      "Error norm 11.9970076243\n",
      "grads loss 7.05819930855\n",
      "shape of grads (18,)\n",
      "Error norm 26.6833281282\n",
      "grads loss 23.6765466103\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.4288107714\n",
      "shape of grads (18,)\n",
      "Error norm 16.2277561247\n",
      "grads loss 6.96410993407\n",
      "shape of grads (18,)\n",
      "Error norm 1.34223261434e-08\n",
      "grads loss 21.4287785177\n",
      "shape of grads (18,)\n",
      "Error norm 26.6833281283\n",
      "grads loss 23.676572783\n",
      "shape of grads (18,)\n",
      "Error norm 8.36968602441\n",
      "grads loss 12.0829315883\n",
      "shape of grads (18,)\n",
      "Error norm 26.4764045369\n",
      "grads loss 23.2860076516\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.4288007761\n",
      "shape of grads (18,)\n",
      "Error norm 18.0771472369\n",
      "grads loss 8.9354787992\n",
      "shape of grads (18,)\n",
      "Error norm 12.1462872858\n",
      "grads loss 6.8993371897\n",
      "shape of grads (18,)\n",
      "Error norm 17.9993184869\n",
      "grads loss 8.89669403537\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.428784928\n",
      "shape of grads (18,)\n",
      "Error norm 26.6833281283\n",
      "grads loss 23.6765671754\n",
      "shape of grads (18,)\n",
      "Error norm 1.55953911182e-10\n",
      "grads loss 21.4287862097\n",
      "shape of grads (18,)\n",
      "Error norm 26.4764045897\n",
      "grads loss 23.2860179348\n",
      "shape of grads (18,)\n",
      "Error norm 6.9120456885\n",
      "grads loss 14.4839210988\n",
      "shape of grads (18,)\n",
      "Error norm 17.5722049444\n",
      "grads loss 8.28924189608\n",
      "shape of grads (18,)\n",
      "Error norm 12.6402485876\n",
      "grads loss 6.5150100333\n",
      "shape of grads (18,)\n",
      "Error norm 16.6732937631\n",
      "grads loss 7.41108138439\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.4287815038\n",
      "shape of grads (18,)\n",
      "Error norm 26.6833281283\n",
      "grads loss 23.6765712145\n",
      "shape of grads (18,)\n",
      "Error norm 1.78294126396\n",
      "grads loss 20.9412248453\n",
      "shape of grads (18,)\n",
      "Error norm 26.6833281282\n",
      "grads loss 23.6765632329\n",
      "shape of grads (18,)\n",
      "Error norm 7.63114750194\n",
      "grads loss 13.4012744977\n",
      "shape of grads (18,)\n",
      "Error norm 18.0326659957\n",
      "grads loss 8.88910504822\n",
      "shape of grads (18,)\n",
      "Error norm 11.1153184123\n",
      "grads loss 8.0218143921\n",
      "shape of grads (18,)\n",
      "Error norm 26.1816211962\n",
      "grads loss 22.4284019126\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.4288071269\n",
      "shape of grads (18,)\n",
      "Error norm 16.4063881867\n",
      "grads loss 7.12845073156\n",
      "shape of grads (18,)\n",
      "Error norm 10.714106765\n",
      "grads loss 8.40491127045\n",
      "shape of grads (18,)\n",
      "Error norm 26.6833281283\n",
      "grads loss 23.6765481645\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.4288070352\n",
      "shape of grads (18,)\n",
      "Error norm 17.42381368\n",
      "grads loss 8.14115367257\n",
      "shape of grads (18,)\n",
      "Error norm 3.00006024973\n",
      "grads loss 18.5489275616\n",
      "shape of grads (18,)\n",
      "Error norm 26.6833281283\n",
      "grads loss 23.6765655515\n",
      "shape of grads (18,)\n",
      "Error norm 3.35462840443\n",
      "grads loss 18.4418116713\n",
      "shape of grads (18,)\n",
      "Error norm 26.1877138502\n",
      "grads loss 22.2504306687\n",
      "shape of grads (18,)\n",
      "Error norm 3.61777118617\n",
      "grads loss 18.2872909764\n",
      "shape of grads (18,)\n",
      "Error norm 18.3296554027\n",
      "grads loss 9.39133357917\n",
      "shape of grads (18,)\n",
      "Error norm 12.9073254899\n",
      "grads loss 6.30869745379\n",
      "shape of grads (18,)\n",
      "Error norm 14.4466184702\n",
      "grads loss 6.0766545933\n",
      "shape of grads (18,)\n",
      "Error norm 6.79850097903\n",
      "grads loss 11.8121845959\n",
      "shape of grads (18,)\n",
      "Error norm 26.6833281283\n",
      "grads loss 23.6765607424\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.4287963131\n",
      "shape of grads (18,)\n",
      "Error norm 26.6710581985\n",
      "grads loss 23.6676914488\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.4287975833\n",
      "shape of grads (18,)\n",
      "Error norm 19.6921963464\n",
      "grads loss 11.236978558\n",
      "shape of grads (18,)\n",
      "Error norm 10.8763165504\n",
      "grads loss 8.32432518033\n",
      "shape of grads (18,)\n",
      "Error norm 18.6353634461\n",
      "grads loss 10.0112941203\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.428784617\n",
      "shape of grads (18,)\n",
      "Error norm 26.6833281283\n",
      "grads loss 23.6765652443\n",
      "shape of grads (18,)\n",
      "Error norm 3.77947565325\n",
      "grads loss 17.5137412427\n",
      "shape of grads (18,)\n",
      "Error norm 25.7281428247\n",
      "grads loss 21.0666498815\n",
      "shape of grads (18,)\n",
      "Error norm 1.73634788496\n",
      "grads loss 20.9593344211\n",
      "shape of grads (18,)\n",
      "Error norm 20.4977275486\n",
      "grads loss 12.4011876366\n",
      "shape of grads (18,)\n",
      "Error norm 12.569526808\n",
      "grads loss 6.57469079173\n",
      "shape of grads (18,)\n",
      "Error norm 14.1773112876\n",
      "grads loss 6.03050074736\n",
      "shape of grads (18,)\n",
      "Error norm 13.4105191413\n",
      "grads loss 6.00206142843\n",
      "shape of grads (18,)\n",
      "Error norm 23.6619007991\n",
      "grads loss 15.1847763436\n",
      "shape of grads (18,)\n",
      "Error norm 6.80591749139\n",
      "grads loss 13.744030354\n",
      "shape of grads (18,)\n",
      "Error norm 25.7109172648\n",
      "grads loss 21.0479858928\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.4287956451\n",
      "shape of grads (18,)\n",
      "Error norm 20.1191169391\n",
      "grads loss 11.8544715635\n",
      "shape of grads (18,)\n",
      "Error norm 11.3016213824\n",
      "grads loss 7.75558945748\n",
      "shape of grads (18,)\n",
      "Error norm 16.8047051415\n",
      "grads loss 7.57055355301\n",
      "shape of grads (18,)\n",
      "Error norm 1.35592733848e-14\n",
      "grads loss 21.4287791266\n",
      "shape of grads (18,)\n",
      "Error norm 26.6833281283\n",
      "grads loss 23.6765709067\n",
      "shape of grads (18,)\n",
      "Error norm 7.90459545471\n",
      "grads loss 11.7758696468\n",
      "shape of grads (18,)\n",
      "Error norm 25.0417314709\n",
      "grads loss 19.8933799295\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.4287968834\n",
      "shape of grads (18,)\n",
      "Error norm 20.8027620344\n",
      "grads loss 12.8990547603\n",
      "shape of grads (18,)\n",
      "Error norm 8.50482850741\n",
      "grads loss 11.0659632951\n",
      "shape of grads (18,)\n",
      "Error norm 22.4139238803\n",
      "grads loss 15.2967136526\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.4287904877\n",
      "shape of grads (18,)\n",
      "Error norm 26.4764045897\n",
      "grads loss 23.2860143147\n",
      "shape of grads (18,)\n",
      "Error norm 2.22929394174\n",
      "grads loss 19.7179325784\n",
      "shape of grads (18,)\n",
      "Error norm 19.778245835\n",
      "grads loss 11.3670112457\n",
      "shape of grads (18,)\n",
      "Error norm 12.4296049929\n",
      "grads loss 6.65010529377\n",
      "shape of grads (18,)\n",
      "Error norm 14.4550004624\n",
      "grads loss 6.07828784908\n",
      "shape of grads (18,)\n",
      "Error norm 10.2547843683\n",
      "grads loss 7.04686999525\n",
      "shape of grads (18,)\n",
      "Error norm 26.6752236825\n",
      "grads loss 23.6738017249\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.4288086704\n",
      "shape of grads (18,)\n",
      "Error norm 18.7921227933\n",
      "grads loss 10.3079171549\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.4287862622\n",
      "shape of grads (18,)\n",
      "Error norm 26.6833281283\n",
      "grads loss 23.6765640072\n",
      "shape of grads (18,)\n",
      "Error norm 1.41408319056\n",
      "grads loss 21.0655597455\n",
      "shape of grads (18,)\n",
      "Error norm 26.3667249341\n",
      "grads loss 22.8862892737\n",
      "shape of grads (18,)\n",
      "Error norm 6.32336365128\n",
      "grads loss 14.7089197565\n",
      "shape of grads (18,)\n",
      "Error norm 16.5212057661\n",
      "grads loss 7.24766658733\n",
      "shape of grads (18,)\n",
      "Error norm 13.4862251501\n",
      "grads loss 6.06306223327\n",
      "shape of grads (18,)\n",
      "Error norm 14.1459232903\n",
      "grads loss 6.02140296347\n",
      "shape of grads (18,)\n",
      "Error norm 7.75644751228\n",
      "grads loss 17.6290600708\n",
      "shape of grads (18,)\n",
      "Error norm 26.6833281283\n",
      "grads loss 23.6765678412\n",
      "shape of grads (18,)\n",
      "Error norm 4.32505724911\n",
      "grads loss 16.6015250837\n",
      "shape of grads (18,)\n",
      "Error norm 26.4575047534\n",
      "grads loss 23.2136502851\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.4287944145\n",
      "shape of grads (18,)\n",
      "Error norm 19.2989916394\n",
      "grads loss 10.6955417747\n",
      "shape of grads (18,)\n",
      "Error norm 12.9224835061\n",
      "grads loss 6.3058695646\n",
      "shape of grads (18,)\n",
      "Error norm 14.1420817712\n",
      "grads loss 6.02392591016\n",
      "shape of grads (18,)\n",
      "Error norm 11.838075396\n",
      "grads loss 5.61849678964\n",
      "shape of grads (18,)\n",
      "Error norm 25.7583514519\n",
      "grads loss 22.2683080737\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.4288101234\n",
      "shape of grads (18,)\n",
      "Error norm 16.2897176323\n",
      "grads loss 7.06895117447\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.4287784487\n",
      "shape of grads (18,)\n",
      "Error norm 26.6833281283\n",
      "grads loss 23.6765721424\n",
      "shape of grads (18,)\n",
      "Error norm 7.67640503432\n",
      "grads loss 11.8424727053\n",
      "shape of grads (18,)\n",
      "Error norm 26.4608032014\n",
      "grads loss 23.2683978464\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.4288011964\n",
      "shape of grads (18,)\n",
      "Error norm 17.7534779474\n",
      "grads loss 8.55767803363\n",
      "shape of grads (18,)\n",
      "Error norm 12.1951002697\n",
      "grads loss 6.75191520458\n",
      "shape of grads (18,)\n",
      "Error norm 17.5741177123\n",
      "grads loss 8.67079071885\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.4287843547\n",
      "shape of grads (18,)\n",
      "Error norm 26.6833281283\n",
      "grads loss 23.6765671719\n",
      "shape of grads (18,)\n",
      "Error norm 1.48952049195e-15\n",
      "grads loss 21.4287856364\n",
      "shape of grads (18,)\n",
      "Error norm 26.4764052495\n",
      "grads loss 23.286279869\n",
      "shape of grads (18,)\n",
      "Error norm 6.55736808496\n",
      "grads loss 14.0328086797\n",
      "shape of grads (18,)\n",
      "Error norm 17.3390464294\n",
      "grads loss 8.08708545458\n",
      "shape of grads (18,)\n",
      "Error norm 12.6067767768\n",
      "grads loss 6.44448959328\n",
      "shape of grads (18,)\n",
      "Error norm 16.0132907255\n",
      "grads loss 6.90550799061\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.4287801589\n",
      "shape of grads (18,)\n",
      "Error norm 26.6833281283\n",
      "grads loss 23.6765722388\n",
      "shape of grads (18,)\n",
      "Error norm 3.46410161454\n",
      "grads loss 17.6280088097\n",
      "shape of grads (18,)\n",
      "Error norm 26.6833277163\n",
      "grads loss 23.6765589685\n",
      "shape of grads (18,)\n",
      "Error norm 1.41421356152\n",
      "grads loss 21.0655292597\n",
      "shape of grads (18,)\n",
      "Error norm 21.922301939\n",
      "grads loss 14.4859978936\n",
      "shape of grads (18,)\n",
      "Error norm 10.9629087718\n",
      "grads loss 7.97506250109\n",
      "shape of grads (18,)\n",
      "Error norm 15.4632790596\n",
      "grads loss 6.46288685447\n",
      "shape of grads (18,)\n",
      "Error norm 9.81616679383\n",
      "grads loss 7.76626877949\n",
      "shape of grads (18,)\n",
      "Error norm 26.6832122899\n",
      "grads loss 23.6765150687\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.4288070173\n",
      "shape of grads (18,)\n",
      "Error norm 18.401960626\n",
      "grads loss 9.64414843716\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.4287830038\n",
      "shape of grads (18,)\n",
      "Error norm 26.6833281283\n",
      "grads loss 23.6765659133\n",
      "shape of grads (18,)\n",
      "Error norm 4.91381850161\n",
      "grads loss 15.6706149982\n",
      "shape of grads (18,)\n",
      "Error norm 23.3697737712\n",
      "grads loss 16.7680101938\n",
      "shape of grads (18,)\n",
      "Error norm 3.00000000146\n",
      "grads loss 18.5505425195\n",
      "shape of grads (18,)\n",
      "Error norm 25.2038271171\n",
      "grads loss 20.1710557609\n",
      "shape of grads (18,)\n",
      "Error norm 4.41528880969\n",
      "grads loss 16.4413006651\n",
      "shape of grads (18,)\n",
      "Error norm 18.4783841589\n",
      "grads loss 9.67095977425\n",
      "shape of grads (18,)\n",
      "Error norm 11.7090380558\n",
      "grads loss 7.03361561103\n",
      "shape of grads (18,)\n",
      "Error norm 17.5493498711\n",
      "grads loss 8.62028076838\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.4287831658\n",
      "shape of grads (18,)\n",
      "Error norm 26.6833281283\n",
      "grads loss 23.6765674552\n",
      "shape of grads (18,)\n",
      "Error norm 2.99999013959\n",
      "grads loss 18.550578761\n",
      "shape of grads (18,)\n",
      "Error norm 26.4764045897\n",
      "grads loss 23.2860138475\n",
      "shape of grads (18,)\n",
      "Error norm 3.0\n",
      "grads loss 18.550581414\n",
      "shape of grads (18,)\n",
      "Error norm 18.4673144522\n",
      "grads loss 9.64812742614\n",
      "shape of grads (18,)\n",
      "Error norm 12.9094789709\n",
      "grads loss 6.25974718245\n",
      "shape of grads (18,)\n",
      "Error norm 14.3094737013\n",
      "grads loss 6.03078208044\n",
      "shape of grads (18,)\n",
      "Error norm 9.0083521626\n",
      "grads loss 8.29993602902\n",
      "shape of grads (18,)\n",
      "Error norm 26.6584634859\n",
      "grads loss 23.6639989601\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.4288045866\n",
      "shape of grads (18,)\n",
      "Error norm 25.9662753912\n",
      "grads loss 22.3485792377\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.4288039102\n",
      "shape of grads (18,)\n",
      "Error norm 17.2733545918\n",
      "grads loss 8.04689595452\n",
      "shape of grads (18,)\n",
      "Error norm 10.122201885\n",
      "grads loss 8.03531560282\n",
      "shape of grads (18,)\n",
      "Error norm 26.4213221715\n",
      "grads loss 23.2831013725\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.428807158\n",
      "shape of grads (18,)\n",
      "Error norm 15.9645171389\n",
      "grads loss 6.80539443777\n",
      "shape of grads (18,)\n",
      "Error norm 10.3440788386\n",
      "grads loss 7.45382235354\n",
      "shape of grads (18,)\n",
      "Error norm 26.6327590994\n",
      "grads loss 23.6317265376\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.428808117\n",
      "shape of grads (18,)\n",
      "Error norm 16.3203865155\n",
      "grads loss 7.08078096837\n",
      "shape of grads (18,)\n",
      "Error norm 6.55178737536\n",
      "grads loss 12.3758094392\n",
      "shape of grads (18,)\n",
      "Error norm 26.6833281283\n",
      "grads loss 23.6765570058\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.4287971223\n",
      "shape of grads (18,)\n",
      "Error norm 25.3759518485\n",
      "grads loss 20.5621512204\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.4287937424\n",
      "shape of grads (18,)\n",
      "Error norm 20.6286843438\n",
      "grads loss 12.7057949799\n",
      "shape of grads (18,)\n",
      "Error norm 10.9584688572\n",
      "grads loss 7.71112778054\n",
      "shape of grads (18,)\n",
      "Error norm 16.1502978901\n",
      "grads loss 6.96063151754\n",
      "shape of grads (18,)\n",
      "Error norm 4.33540230638\n",
      "grads loss 15.8493686466\n",
      "shape of grads (18,)\n",
      "Error norm 26.6833281283\n",
      "grads loss 23.6765635493\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.428789635\n",
      "shape of grads (18,)\n",
      "Error norm 26.4764045897\n",
      "grads loss 23.2860142425\n",
      "shape of grads (18,)\n",
      "Error norm 3.46410161514\n",
      "grads loss 17.6280198448\n",
      "shape of grads (18,)\n",
      "Error norm 18.4097609321\n",
      "grads loss 9.5554806013\n",
      "shape of grads (18,)\n",
      "Error norm 12.2530507565\n",
      "grads loss 6.57557362256\n",
      "shape of grads (18,)\n",
      "Error norm 15.4938167604\n",
      "grads loss 6.49380449215\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.4287769147\n",
      "shape of grads (18,)\n",
      "Error norm 26.6833281283\n",
      "grads loss 23.6765736596\n",
      "shape of grads (18,)\n",
      "Error norm 8.29840445005\n",
      "grads loss 10.2691628314\n",
      "shape of grads (18,)\n",
      "Error norm 26.2453682259\n",
      "grads loss 22.9224806821\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.4288029529\n",
      "shape of grads (18,)\n",
      "Error norm 17.3311159629\n",
      "grads loss 8.12316057052\n",
      "shape of grads (18,)\n",
      "Error norm 10.5854226399\n",
      "grads loss 7.42524327441\n",
      "shape of grads (18,)\n",
      "Error norm 23.6886722545\n",
      "grads loss 18.8688301135\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.4288013519\n",
      "shape of grads (18,)\n",
      "Error norm 21.2354390758\n",
      "grads loss 13.7835578183\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.428786253\n",
      "shape of grads (18,)\n",
      "Error norm 26.5384889016\n",
      "grads loss 23.4643146489\n",
      "shape of grads (18,)\n",
      "Error norm 5.08313100238\n",
      "grads loss 15.3874291411\n",
      "shape of grads (18,)\n",
      "Error norm 18.3801951525\n",
      "grads loss 9.53740855684\n",
      "shape of grads (18,)\n",
      "Error norm 10.6517427137\n",
      "grads loss 7.54679937198\n",
      "shape of grads (18,)\n",
      "Error norm 19.3070168797\n",
      "grads loss 11.116485429\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.4287876146\n",
      "shape of grads (18,)\n",
      "Error norm 26.6833281283\n",
      "grads loss 23.6765618985\n",
      "shape of grads (18,)\n",
      "Error norm 0.0\n",
      "grads loss 21.4287888963\n",
      "shape of grads (18,)\n",
      "Error norm 25.7410325173\n",
      "grads loss 21.2097215131\n"
     ]
    }
   ],
   "source": [
    "subset = clean_data[0]\n",
    "#c = stochastic_gdescent(subset ,0.01,0.001, 1000)\n",
    "d = batch_logistic_descent(subset, 1, 0.01, 1, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6c5d8ccc90>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEACAYAAABMEua6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPy0xoSQ5yM27AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6c68ee6450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>434</td>\n",
       "      <td>434</td>\n",
       "      <td>434</td>\n",
       "      <td>434</td>\n",
       "      <td>334</td>\n",
       "      <td>434</td>\n",
       "      <td>434</td>\n",
       "      <td>434</td>\n",
       "      <td>434</td>\n",
       "      <td>434</td>\n",
       "      <td>434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "      <td>231</td>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          PassengerId  Pclass  Name  Sex  Age  SibSp  Parch  Ticket  Fare  \\\n",
       "Survived                                                                    \n",
       "0                 434     434   434  434  334    434    434     434   434   \n",
       "1                 278     278   278  278  231    278    278     278   278   \n",
       "\n",
       "          Cabin  Embarked  \n",
       "Survived                   \n",
       "0           434       434  \n",
       "1           278       277  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.groupby('Survived').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#stochastic approach using linear regression method\n",
    "#Plot the value and compute for the average decision boundary of the survivor statstics\n",
    "def stochastic_gdescent (data, step_size, tolerence, max_iters=10000, theta=None):\n",
    "    label = data.iloc[:,-1]\n",
    "    if theta is None:\n",
    "        theta = np.zeros(data.shape[1]-1)\n",
    "    for x in range(0, max_iters):\n",
    "        print x\n",
    "        i = np.random.randint(low=0, high=data.shape[0], size=None)\n",
    "        error = theta.dot(data.iloc[i,:-1]) - label[i]\n",
    "        print \"Error \", error\n",
    "        for y in range(0,len(theta)):\n",
    "            delta_weight =  step_size * error * data.iloc[i,y]\n",
    "            theta[y] = theta[y] - delta_weight\n",
    "        #Report every 100 iteration to see the gradient and add data for plot\n",
    "        if (x % 100 == 0) :\n",
    "            #print \"Data point of x\" , x, \" is \", data.iloc[i]\n",
    "            print \"Iteration\", x\n",
    "            print \"Error in prediction at this point\", error\n",
    "            print \"Current theta\", theta\n",
    "            #a[x/100] = (np.sum(data.dot(theta) - label **2)/10000) #error (normalize)\n",
    "        if (error **2 < tolerence) and x > 200:\n",
    "            print \"Error reached termination condition\"\n",
    "            return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Batch approach using linear regression method\n",
    "def batch_gdescent(data, step_size, tolerence, max_iteration=10000, theta=None):\n",
    "    label = data.iloc[:,-1]\n",
    "    print label.shape\n",
    "    if theta is None:\n",
    "        theta = np.zeros(data.shape[1]-1)\n",
    "    not_convergence = True\n",
    "    gradient_magnitude = 0\n",
    "    iteration = 0\n",
    "    while not_convergence:\n",
    "        error = data.iloc[:,:-1].dot(theta) - label\n",
    "        print \"sum of error\", error.sum()\n",
    "        for y in range(0, len(theta)):\n",
    "            delta_weight = (error.dot(data.iloc[:,y]))\n",
    "            print \"delta sum\", delta_weight\n",
    "            theta[y] -= (step_size * delta_weight)\n",
    "            gradient_magnitude += delta_weight\n",
    "        print \"theta is\", theta\n",
    "        if gradient_magnitude ** 2 < tolerence:\n",
    "            print theta\n",
    "            not_convergence = False\n",
    "        iteration +=1\n",
    "        print \"iteration\", iteration\n",
    "        gradient_mangitude = 0\n",
    "        print \"error\" , error\n",
    "        if iteration > max_iteration:\n",
    "            break\n",
    "    return theta            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "shape of grads (18,)\n",
      "Error norm 1.58113883008\n",
      "grads loss 3.85939572781\n",
      "shape of grads (18,)\n",
      "Error norm 2.25281958021\n",
      "grads loss 8.03867441554\n",
      "shape of grads (18,)\n",
      "Error norm 0.561899305367\n",
      "grads loss 18.5210256774\n",
      "shape of grads (18,)\n",
      "Error norm 3.02378505336\n",
      "grads loss 16.1979878054\n",
      "shape of grads (18,)\n",
      "Error norm 1.09060679691\n",
      "grads loss 11.20186771\n",
      "shape of grads (18,)\n",
      "Error norm 2.69876043361\n",
      "grads loss 13.0279257115\n",
      "shape of grads (18,)\n",
      "Error norm 0.853492190808\n",
      "grads loss 15.6052964042\n",
      "shape of grads (18,)\n",
      "Error norm 2.8206415649\n",
      "grads loss 14.2154286828\n",
      "shape of grads (18,)\n",
      "Error norm 1.27420683088\n",
      "grads loss 9.73311706732\n",
      "shape of grads (18,)\n",
      "Error norm 2.46201473151\n",
      "grads loss 10.2330863227\n",
      "shape of grads (18,)\n",
      "Error norm 1.08806041455\n",
      "grads loss 12.1395072398\n",
      "shape of grads (18,)\n",
      "Error norm 2.62319538475\n",
      "grads loss 12.143189312\n",
      "shape of grads (18,)\n",
      "Error norm 1.12212863361\n",
      "grads loss 11.7444653359\n",
      "shape of grads (18,)\n",
      "Error norm 2.58062852498\n",
      "grads loss 11.6559500162\n",
      "shape of grads (18,)\n",
      "Error norm 1.14469723527\n",
      "grads loss 11.3875854915\n",
      "shape of grads (18,)\n",
      "Error norm 2.55835612158\n",
      "grads loss 11.4042316403\n",
      "shape of grads (18,)\n",
      "Error norm 1.1305017898\n",
      "grads loss 11.5198784227\n",
      "shape of grads (18,)\n",
      "Error norm 2.56827068732\n",
      "grads loss 11.5249341347\n",
      "shape of grads (18,)\n",
      "Error norm 1.12712421558\n",
      "grads loss 11.5277252977\n",
      "shape of grads (18,)\n",
      "Error norm 2.56728652905\n",
      "grads loss 11.5173036977\n",
      "shape of grads (18,)\n",
      "Error norm 1.12721822529\n",
      "grads loss 11.4844940572\n",
      "shape of grads (18,)\n",
      "Error norm 2.56375102012\n",
      "grads loss 11.4808068266\n",
      "shape of grads (18,)\n",
      "Error norm 1.12434927557\n",
      "grads loss 11.479010797\n",
      "shape of grads (18,)\n",
      "Error norm 2.5629546592\n",
      "grads loss 11.4761339422\n",
      "shape of grads (18,)\n",
      "Error norm 1.12178813742\n",
      "grads loss 11.47141195\n",
      "shape of grads (18,)\n",
      "Error norm 2.561800411\n",
      "grads loss 11.4671534171\n",
      "shape of grads (18,)\n",
      "Error norm 1.11971817171\n",
      "grads loss 11.4576788983\n",
      "shape of grads (18,)\n",
      "Error norm 2.56026060189\n",
      "grads loss 11.4537296394\n",
      "shape of grads (18,)\n",
      "Error norm 1.11750265156\n",
      "grads loss 11.4460644417\n",
      "shape of grads (18,)\n",
      "Error norm 2.55889396978\n",
      "grads loss 11.442326272\n",
      "shape of grads (18,)\n",
      "Error norm 1.11529525731\n",
      "grads loss 11.4348937666\n",
      "shape of grads (18,)\n",
      "Error norm 2.55754353381\n",
      "grads loss 11.4310922365\n",
      "shape of grads (18,)\n",
      "Error norm 1.11316696065\n",
      "grads loss 11.4231767497\n",
      "shape of grads (18,)\n",
      "Error norm 2.55615505819\n",
      "grads loss 11.4194091758\n",
      "shape of grads (18,)\n",
      "Error norm 1.11107005987\n",
      "grads loss 11.4115062386\n",
      "shape of grads (18,)\n",
      "Error norm 2.55477309298\n",
      "grads loss 11.4077949402\n",
      "shape of grads (18,)\n",
      "Error norm 1.10900600445\n",
      "grads loss 11.3998966142\n",
      "shape of grads (18,)\n",
      "Error norm 2.55339524151\n",
      "grads loss 11.3962179802\n",
      "shape of grads (18,)\n",
      "Error norm 1.10698363088\n",
      "grads loss 11.3882413221\n",
      "shape of grads (18,)\n",
      "Error norm 2.55201466764\n",
      "grads loss 11.384598465\n",
      "shape of grads (18,)\n",
      "Error norm 1.10500059691\n",
      "grads loss 11.3765735079\n",
      "shape of grads (18,)\n",
      "Error norm 2.55063434309\n",
      "grads loss 11.3729702979\n",
      "shape of grads (18,)\n",
      "Error norm 1.10305632497\n",
      "grads loss 11.3649085537\n",
      "shape of grads (18,)\n",
      "Error norm 2.54925525037\n",
      "grads loss 11.3613437027\n",
      "shape of grads (18,)\n",
      "Error norm 1.1011517241\n",
      "grads loss 11.3532411653\n",
      "shape of grads (18,)\n",
      "Error norm 2.54787712956\n",
      "grads loss 11.349714667\n",
      "shape of grads (18,)\n",
      "Error norm 1.09928688485\n",
      "grads loss 11.3415750785\n",
      "shape of grads (18,)\n",
      "Error norm 2.54650043153\n",
      "grads loss 11.3380874856\n",
      "shape of grads (18,)\n",
      "Error norm 1.09746178447\n",
      "grads loss 11.3299152186\n",
      "shape of grads (18,)\n",
      "Error norm 2.54512562899\n",
      "grads loss 11.3264666545\n",
      "shape of grads (18,)\n",
      "Error norm 1.09567651822\n",
      "grads loss 11.3182644052\n",
      "shape of grads (18,)\n",
      "Error norm 2.54375304826\n",
      "grads loss 11.3148549755\n",
      "shape of grads (18,)\n",
      "Error norm 1.0939311051\n",
      "grads loss 11.3066257272\n",
      "shape of grads (18,)\n",
      "Error norm 2.54238304387\n",
      "grads loss 11.3032555901\n",
      "shape of grads (18,)\n",
      "Error norm 1.09222550088\n",
      "grads loss 11.2950024921\n",
      "shape of grads (18,)\n",
      "Error norm 2.54101598089\n",
      "grads loss 11.2916717628\n",
      "shape of grads (18,)\n",
      "Error norm 1.0905596332\n",
      "grads loss 11.2833977937\n",
      "shape of grads (18,)\n",
      "Error norm 2.53965220677\n",
      "grads loss 11.2801065586\n",
      "shape of grads (18,)\n",
      "Error norm 1.08893339162\n",
      "grads loss 11.2718146391\n",
      "shape of grads (18,)\n",
      "Error norm 2.53829206211\n",
      "grads loss 11.2685629706\n",
      "shape of grads (18,)\n",
      "Error norm 1.08734662612\n",
      "grads loss 11.2602559932\n",
      "shape of grads (18,)\n",
      "Error norm 2.5369358827\n",
      "grads loss 11.2570439441\n",
      "shape of grads (18,)\n",
      "Error norm 1.08579915173\n",
      "grads loss 11.2487247409\n",
      "shape of grads (18,)\n",
      "Error norm 2.5355839966\n",
      "grads loss 11.245552344\n",
      "shape of grads (18,)\n",
      "Error norm 1.08429075\n",
      "grads loss 11.2372236879\n",
      "shape of grads (18,)\n",
      "Error norm 2.53423672431\n",
      "grads loss 11.2340909582\n",
      "shape of grads (18,)\n",
      "Error norm 1.08282117015\n",
      "grads loss 11.2257555663\n",
      "shape of grads (18,)\n",
      "Error norm 2.532894379\n",
      "grads loss 11.2226625013\n",
      "shape of grads (18,)\n",
      "Error norm 1.08139013095\n",
      "grads loss 11.214323032\n",
      "shape of grads (18,)\n",
      "Error norm 2.53155726623\n",
      "grads loss 11.2112696123\n",
      "shape of grads (18,)\n",
      "Error norm 1.07999732242\n",
      "grads loss 11.2029286636\n",
      "shape of grads (18,)\n",
      "Error norm 2.53022568376\n",
      "grads loss 11.1999148536\n",
      "shape of grads (18,)\n",
      "Error norm 1.07864240752\n",
      "grads loss 11.1915749628\n",
      "shape of grads (18,)\n",
      "Error norm 2.52889992145\n",
      "grads loss 11.1886007114\n",
      "shape of grads (18,)\n",
      "Error norm 1.07732502379\n",
      "grads loss 11.1802643541\n",
      "shape of grads (18,)\n",
      "Error norm 2.52758026123\n",
      "grads loss 11.1773295952\n",
      "shape of grads (18,)\n",
      "Error norm 1.076044785\n",
      "grads loss 11.1689991844\n",
      "shape of grads (18,)\n",
      "Error norm 2.52626697691\n",
      "grads loss 11.1661038377\n",
      "shape of grads (18,)\n",
      "Error norm 1.07480128281\n",
      "grads loss 11.1577817237\n",
      "shape of grads (18,)\n",
      "Error norm 2.52496033418\n",
      "grads loss 11.1549256951\n",
      "shape of grads (18,)\n",
      "Error norm 1.07359408835\n",
      "grads loss 11.1466141644\n",
      "shape of grads (18,)\n",
      "Error norm 2.52366059052\n",
      "grads loss 11.143797347\n",
      "shape of grads (18,)\n",
      "Error norm 1.0724227538\n",
      "grads loss 11.1354986223\n",
      "shape of grads (18,)\n",
      "Error norm 2.52236799515\n",
      "grads loss 11.1327208966\n",
      "shape of grads (18,)\n",
      "Error norm 1.07128681392\n",
      "grads loss 11.1244371364\n",
      "shape of grads (18,)\n",
      "Error norm 2.521082789\n",
      "grads loss 11.1216983711\n",
      "shape of grads (18,)\n",
      "Error norm 1.0701857876\n",
      "grads loss 11.1134316698\n",
      "shape of grads (18,)\n",
      "Error norm 2.51980520468\n",
      "grads loss 11.1107317222\n",
      "shape of grads (18,)\n",
      "Error norm 1.06911917925\n",
      "grads loss 11.1024841097\n",
      "shape of grads (18,)\n",
      "Error norm 2.51853546648\n",
      "grads loss 11.0998228265\n",
      "shape of grads (18,)\n",
      "Error norm 1.06808648027\n",
      "grads loss 11.0915962683\n",
      "shape of grads (18,)\n",
      "Error norm 2.51727379035\n",
      "grads loss 11.088973486\n",
      "shape of grads (18,)\n",
      "Error norm 1.06708717041\n",
      "grads loss 11.0807698836\n",
      "shape of grads (18,)\n",
      "Error norm 2.51602038392\n",
      "grads loss 11.078185429\n",
      "shape of grads (18,)\n",
      "Error norm 1.06612071907\n",
      "grads loss 11.0700066197\n",
      "shape of grads (18,)\n",
      "Error norm 2.51477544652\n",
      "grads loss 11.0674603103\n",
      "shape of grads (18,)\n",
      "Error norm 1.06518658665\n",
      "grads loss 11.0593080679\n",
      "shape of grads (18,)\n",
      "Error norm 2.51353916919\n",
      "grads loss 11.0567997127\n",
      "shape of grads (18,)\n",
      "Error norm 1.06428422571\n",
      "grads loss 11.0486757474\n",
      "shape of grads (18,)\n",
      "Error norm 2.51231173476\n",
      "grads loss 11.046205147\n",
      "shape of grads (18,)\n",
      "Error norm 1.06341308216\n",
      "grads loss 11.0381111062\n"
     ]
    }
   ],
   "source": [
    "subset = clean_data[0].iloc[:10]\n",
    "#c = stochastic_gdescent(subset ,0.01,0.001, 1000)\n",
    "d = batch_logistic_descent(subset, 0.01, 0.001, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pclass       -0.073082\n",
      "Sex          -0.212342\n",
      "Age          -0.017271\n",
      "SibSp        -0.173942\n",
      "Parch         0.042677\n",
      "Fare          0.135694\n",
      "Cabin_A       0.000000\n",
      "Cabin_B       0.000000\n",
      "Cabin_C       0.052148\n",
      "Cabin_D       0.000000\n",
      "Cabin_E      -0.050861\n",
      "Cabin_F       0.000000\n",
      "Cabin_G       0.000000\n",
      "Cabin_T       0.000000\n",
      "Cabin_n      -0.016900\n",
      "Embarked_C    0.044877\n",
      "Embarked_Q   -0.030145\n",
      "Embarked_S   -0.030345\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(subset.iloc[:-5,:-1], subset.iloc[:-5,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method LogisticRegression.densify of LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0)>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.densify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sigmod helper function for activation\n",
    "def sigmod(x):\n",
    "    if x < 0:\n",
    "        return 1 - 1/(1+math.exp(x))\n",
    "    else:\n",
    "        return (1.0 / ( 1 + math.exp(-1 * x)))\n",
    "\n",
    "def logistic_regression (train_data, label, max_iterations, lamda, theta=None, threshold=0.1, alpha=0.01):\n",
    "    if theta is None:\n",
    "        theta = np.zeros(train_data.shape[1], dtype=np.float)\n",
    "        print theta\n",
    "    n = train_data.shape[1] #size of the data\n",
    "    m = train_data.shape[0] #number of the features\n",
    "    for iteration in range(0, max_iterations):\n",
    "        hx = train_data.dot(theta).apply(lambda x: sigmod(x))\n",
    "        #print \"hx is \", hx\n",
    "        gradient = (1.0/m) * train_data.T.dot(hx - label) + (float(lamda)/m * theta)\n",
    "        #print \"gradient\", gradient\n",
    "        theta = theta - (alpha * gradient)\n",
    "        \n",
    "        loss = np.linalg.norm(gradient)\n",
    "        if loss < threshold:\n",
    "            break;\n",
    "        print iteration, \"with error of \", loss\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0 with error of  0.305811393028\n",
      "1 with error of  0.303683770823\n",
      "2 with error of  0.301572416273\n",
      "3 with error of  0.299477232862\n",
      "4 with error of  0.297398123733\n",
      "5 with error of  0.295334991714\n",
      "6 with error of  0.29328773936\n",
      "7 with error of  0.291256268974\n",
      "8 with error of  0.289240482647\n",
      "9 with error of  0.287240282282\n",
      "10 with error of  0.285255569622\n",
      "11 with error of  0.283286246284\n",
      "12 with error of  0.28133221378\n",
      "13 with error of  0.279393373545\n",
      "14 with error of  0.277469626967\n",
      "15 with error of  0.275560875403\n",
      "16 with error of  0.273667020211\n",
      "17 with error of  0.271787962769\n",
      "18 with error of  0.269923604499\n",
      "19 with error of  0.268073846887\n",
      "20 with error of  0.266238591507\n",
      "21 with error of  0.264417740036\n",
      "22 with error of  0.262611194281\n",
      "23 with error of  0.260818856192\n",
      "24 with error of  0.259040627883\n",
      "25 with error of  0.257276411647\n",
      "26 with error of  0.255526109976\n",
      "27 with error of  0.253789625578\n",
      "28 with error of  0.252066861389\n",
      "29 with error of  0.250357720591\n",
      "30 with error of  0.248662106624\n",
      "31 with error of  0.246979923206\n",
      "32 with error of  0.24531107434\n",
      "33 with error of  0.243655464329\n",
      "34 with error of  0.24201299779\n",
      "35 with error of  0.240383579665\n",
      "36 with error of  0.238767115232\n",
      "37 with error of  0.237163510119\n",
      "38 with error of  0.235572670308\n",
      "39 with error of  0.233994502152\n",
      "40 with error of  0.232428912381\n",
      "41 with error of  0.230875808113\n",
      "42 with error of  0.229335096859\n",
      "43 with error of  0.227806686538\n",
      "44 with error of  0.226290485478\n",
      "45 with error of  0.224786402429\n",
      "46 with error of  0.223294346567\n",
      "47 with error of  0.221814227502\n",
      "48 with error of  0.220345955284\n",
      "49 with error of  0.218889440411\n",
      "50 with error of  0.21744459383\n",
      "51 with error of  0.216011326951\n",
      "52 with error of  0.214589551641\n",
      "53 with error of  0.213179180241\n",
      "54 with error of  0.211780125559\n",
      "55 with error of  0.210392300883\n",
      "56 with error of  0.209015619982\n",
      "57 with error of  0.207649997106\n",
      "58 with error of  0.206295346997\n",
      "59 with error of  0.204951584885\n",
      "60 with error of  0.203618626497\n",
      "61 with error of  0.202296388053\n",
      "62 with error of  0.200984786276\n",
      "63 with error of  0.199683738389\n",
      "64 with error of  0.198393162118\n",
      "65 with error of  0.197112975695\n",
      "66 with error of  0.195843097858\n",
      "67 with error of  0.194583447856\n",
      "68 with error of  0.193333945446\n",
      "69 with error of  0.192094510894\n",
      "70 with error of  0.190865064982\n",
      "71 with error of  0.189645529\n",
      "72 with error of  0.188435824754\n",
      "73 with error of  0.187235874561\n",
      "74 with error of  0.186045601252\n",
      "75 with error of  0.184864928174\n",
      "76 with error of  0.183693779183\n",
      "77 with error of  0.182532078653\n",
      "78 with error of  0.181379751468\n",
      "79 with error of  0.180236723024\n",
      "80 with error of  0.179102919232\n",
      "81 with error of  0.177978266511\n",
      "82 with error of  0.176862691792\n",
      "83 with error of  0.175756122514\n",
      "84 with error of  0.174658486627\n",
      "85 with error of  0.173569712587\n",
      "86 with error of  0.172489729353\n",
      "87 with error of  0.171418466394\n",
      "88 with error of  0.170355853679\n",
      "89 with error of  0.169301821678\n",
      "90 with error of  0.168256301363\n",
      "91 with error of  0.167219224205\n",
      "92 with error of  0.16619052217\n",
      "93 with error of  0.16517012772\n",
      "94 with error of  0.164157973809\n",
      "95 with error of  0.163153993883\n",
      "96 with error of  0.162158121877\n",
      "97 with error of  0.161170292213\n",
      "98 with error of  0.160190439799\n",
      "99 with error of  0.159218500023\n",
      "100 with error of  0.158254408755\n",
      "101 with error of  0.157298102345\n",
      "102 with error of  0.156349517616\n",
      "103 with error of  0.155408591865\n",
      "104 with error of  0.154475262863\n",
      "105 with error of  0.153549468847\n",
      "106 with error of  0.15263114852\n",
      "107 with error of  0.151720241051\n",
      "108 with error of  0.150816686067\n",
      "109 with error of  0.149920423656\n",
      "110 with error of  0.149031394362\n",
      "111 with error of  0.148149539181\n",
      "112 with error of  0.14727479956\n",
      "113 with error of  0.146407117396\n",
      "114 with error of  0.145546435027\n",
      "115 with error of  0.144692695238\n",
      "116 with error of  0.143845841253\n",
      "117 with error of  0.14300581673\n",
      "118 with error of  0.142172565766\n",
      "119 with error of  0.141346032885\n",
      "120 with error of  0.140526163044\n",
      "121 with error of  0.139712901623\n",
      "122 with error of  0.138906194427\n",
      "123 with error of  0.13810598768\n",
      "124 with error of  0.137312228025\n",
      "125 with error of  0.136524862518\n",
      "126 with error of  0.135743838628\n",
      "127 with error of  0.134969104234\n",
      "128 with error of  0.13420060762\n",
      "129 with error of  0.133438297474\n",
      "130 with error of  0.132682122883\n",
      "131 with error of  0.131932033334\n",
      "132 with error of  0.131187978708\n",
      "133 with error of  0.130449909278\n",
      "134 with error of  0.129717775707\n",
      "135 with error of  0.128991529043\n",
      "136 with error of  0.128271120719\n",
      "137 with error of  0.127556502549\n",
      "138 with error of  0.126847626724\n",
      "139 with error of  0.12614444581\n",
      "140 with error of  0.125446912747\n",
      "141 with error of  0.124754980844\n",
      "142 with error of  0.124068603778\n",
      "143 with error of  0.123387735587\n",
      "144 with error of  0.122712330673\n",
      "145 with error of  0.122042343797\n",
      "146 with error of  0.121377730075\n",
      "147 with error of  0.120718444977\n",
      "148 with error of  0.120064444323\n",
      "149 with error of  0.11941568428\n",
      "150 with error of  0.118772121362\n",
      "151 with error of  0.118133712426\n",
      "152 with error of  0.117500414665\n",
      "153 with error of  0.116872185615\n",
      "154 with error of  0.116248983142\n",
      "155 with error of  0.115630765447\n",
      "156 with error of  0.115017491058\n",
      "157 with error of  0.114409118832\n",
      "158 with error of  0.113805607951\n",
      "159 with error of  0.113206917915\n",
      "160 with error of  0.112613008548\n",
      "161 with error of  0.112023839988\n",
      "162 with error of  0.111439372688\n",
      "163 with error of  0.110859567413\n",
      "164 with error of  0.110284385237\n",
      "165 with error of  0.109713787542\n",
      "166 with error of  0.109147736014\n",
      "167 with error of  0.10858619264\n",
      "168 with error of  0.108029119709\n",
      "169 with error of  0.107476479806\n",
      "170 with error of  0.106928235811\n",
      "171 with error of  0.106384350897\n",
      "172 with error of  0.105844788528\n",
      "173 with error of  0.105309512456\n",
      "174 with error of  0.104778486718\n",
      "175 with error of  0.104251675635\n",
      "176 with error of  0.10372904381\n",
      "177 with error of  0.103210556124\n",
      "178 with error of  0.102696177736\n",
      "179 with error of  0.102185874079\n",
      "180 with error of  0.10167961086\n",
      "181 with error of  0.101177354054\n",
      "182 with error of  0.100679069907\n",
      "183 with error of  0.100184724929\n"
     ]
    }
   ],
   "source": [
    "parameter = logistic_regression(train_norm.iloc[:,:-1], clean_data[0].iloc[:,-1], 10000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pclass       -0.190208\n",
      "Sex          -0.355267\n",
      "Age          -0.040622\n",
      "SibSp        -0.010259\n",
      "Parch         0.005306\n",
      "Fare          0.021314\n",
      "Cabin_A       0.004837\n",
      "Cabin_B       0.034610\n",
      "Cabin_C       0.019889\n",
      "Cabin_D       0.028855\n",
      "Cabin_E       0.024167\n",
      "Cabin_F       0.012620\n",
      "Cabin_G       0.000811\n",
      "Cabin_T      -0.001672\n",
      "Cabin_n      -0.183449\n",
      "Embarked_C    0.052320\n",
      "Embarked_Q    0.004892\n",
      "Embarked_S   -0.118508\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cross Validation\n",
    "#Different type of split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
